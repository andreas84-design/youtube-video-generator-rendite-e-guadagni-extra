import os
import base64
import json
import tempfile
import subprocess
import uuid
import datetime as dt
import requests
from flask import Flask, request, jsonify
import boto3
from botocore.config import Config
import math
import random
from threading import Thread
import logging
import gspread
from google.oauth2.service_account import Credentials

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
logger = logging.getLogger(__name__)

# üîß Railway Variables
MAX_DURATION = int(os.getenv('MAX_DURATION', '3600'))
MAX_CONCURRENT = int(os.getenv('MAX_CONCURRENT', '5'))
MAX_CLIPS = int(os.getenv('MAX_CLIPS', '40'))
LOG_RATE = int(os.getenv('LOG_RATE', '100'))

app = Flask(__name__)

# Config R2 (S3 compatibile)
R2_ACCESS_KEY_ID = os.environ.get("R2_ACCESS_KEY_ID")
R2_SECRET_ACCESS_KEY = os.environ.get("R2_SECRET_ACCESS_KEY")
R2_BUCKET_NAME = os.environ.get("R2_BUCKET_NAME")
R2_PUBLIC_BASE_URL = os.environ.get("R2_PUBLIC_BASE_URL")
R2_REGION = os.environ.get("R2_REGION", "auto")
R2_ACCOUNT_ID = os.environ.get("R2_ACCOUNT_ID")

# Pexels / Pixabay API
PEXELS_API_KEY = os.environ.get("PEXELS_API_KEY")
PIXABAY_API_KEY = os.environ.get("PIXABAY_API_KEY")
GOOGLE_CREDENTIALS_JSON = os.environ.get("GOOGLE_CREDENTIALS_JSON", "")

# ‚úÖ ID FISSO RENDITE E GUADAGNI EXTRA
SPREADSHEET_ID = "1wHZL3YvhhmuPt5Mrs_kAdCtIw7WVFchwK3pCfJRadUo"

# üîî Webhook flusso 2 (Rendite e Guadagni Extra)
N8N_WEBHOOK_URL_FLUSSO2 = os.environ.get("N8N_WEBHOOK_URL_RENDITE_GUADAGNI_EXTRA_FLUSSO2", "https://andreas84.app.n8n.cloud/webhook/rendite-e-guadagni-extra-flusso-2-workflow-b")

jobs = {}
MAX_JOBS = 50

def get_gspread_client():
    """Client Google Sheets per update Video_URL"""
    try:
        if not GOOGLE_CREDENTIALS_JSON:
            return None
        creds_dict = json.loads(GOOGLE_CREDENTIALS_JSON)
        scopes = ['https://www.googleapis.com/auth/spreadsheets']
        credentials = Credentials.from_service_account_info(creds_dict, scopes=scopes)
        return gspread.authorize(credentials)
    except Exception as e:
        logger.error(f"Google Sheets client error: {e}")
        return None

def get_s3_client():
    """Client S3 configurato per Cloudflare R2"""
    if R2_ACCOUNT_ID:
        endpoint_url = f"https://{R2_ACCOUNT_ID}.r2.cloudflarestorage.com"
    else:
        endpoint_url = None
    if endpoint_url is None:
        raise RuntimeError("Endpoint R2 non configurato: imposta R2_ACCOUNT_ID in Railway")
    
    session = boto3.session.Session()
    s3_client = session.client(
        service_name="s3",
        region_name=R2_REGION,
        endpoint_url=endpoint_url,
        aws_access_key_id=R2_ACCESS_KEY_ID,
        aws_secret_access_key=R2_SECRET_ACCESS_KEY,
        config=Config(s3={"addressing_style": "virtual"}),
    )
    return s3_client

def cleanup_old_videos(s3_client, current_key):
    """Cancella tutti i video MP4 in R2 TRANNE quello appena caricato"""
    try:
        paginator = s3_client.get_paginator("list_objects_v2")
        pages = paginator.paginate(Bucket=R2_BUCKET_NAME, Prefix="videos/")
        deleted_count = 0
        for page in pages:
            if "Contents" not in page:
                continue
            for obj in page["Contents"]:
                key = obj["Key"]
                if key.endswith(".mp4") and key != current_key:
                    s3_client.delete_object(Bucket=R2_BUCKET_NAME, Key=key)
                    deleted_count += 1
                    print(f"üóëÔ∏è Cancellato vecchio video: {key}", flush=True)
        if deleted_count > 0:
            print(f"‚úÖ Rotazione completata: {deleted_count} video vecchi rimossi", flush=True)
        else:
            print("‚úÖ Nessun video vecchio da rimuovere", flush=True)
    except Exception as e:
        print(f"‚ö†Ô∏è Errore rotazione R2 (video vecchi restano): {str(e)}", flush=True)

def notify_n8n_flusso2(job):
    """Invia webhook a n8n quando il job √® completato."""
    if not N8N_WEBHOOK_URL_FLUSSO2:
        print("‚ö†Ô∏è N8N_WEBHOOK_URL_RENDITE_GUADAGNI_EXTRA_FLUSSO2 non configurata, skip webhook", flush=True)
        return

    try:
        payload = {
            "job_id": job.get("job_id"),
            "video_url": job.get("video_url"),
            "duration": job.get("duration"),
            "clips_used": job.get("clips_used"),
            # dati che arrivano dal flusso 1 nel /generate
            "title": job.get("data", {}).get("title"),
            "description_pro": job.get("data", {}).get("description_pro"),
            "row_id": job.get("row_number") or job.get("data", {}).get("row_id"),
            "keywords": job.get("data", {}).get("keywords"),
            "playlist": job.get("data", {}).get("playlist"),
            "channel": "rendite_guadagni_extra",
        }
        resp = requests.post(N8N_WEBHOOK_URL_FLUSSO2, json=payload, timeout=15)
        print(f"üîî Webhook n8n flusso2 status={resp.status_code}", flush=True)
    except Exception as e:
        print(f"‚ö†Ô∏è Errore invio webhook n8n flusso2: {e}", flush=True)

# -------------------------------------------------
# Mapping SCENA ‚Üí QUERY visiva (canale RENDITE E GUADAGNI EXTRA)
# -------------------------------------------------
def pick_visual_query(context: str, keywords_text: str = "") -> str:
    """Query ottimizzate per B-roll MONEY/BUSINESS: passive income, entrepreneur, freelance, side hustle, money, online work."""
    ctx = (context or "").lower()
    kw = (keywords_text or "").lower()
    
    base = "money business passive income entrepreneur laptop online work side hustle freelance"
    
    # Passive Income / Rendite Passive
    if any(w in ctx for w in ["passiv", "rendita", "automatico", "passive income", "reddito passivo"]):
        return "passive income money laptop online business, automated income streams, residual income working"
    
    # Side Hustle / Lavoro Extra
    if any(w in ctx for w in ["side hustle", "secondo lavoro", "extra", "arrotondare"]):
        return "side hustle working laptop coffee shop, freelance online work home office, extra income business"
    
    # Freelance / Autonomo
    if any(w in ctx for w in ["freelance", "autonomo", "professione online", "nomade digitale"]):
        return "freelancer laptop remote work travel, digital nomad working beach cafe, online professional workspace"
    
    # E-commerce / Dropshipping / Vendita Online
    if any(w in ctx for w in ["ecommerce", "dropshipping", "vendere", "negozio online", "prodotti"]):
        return "ecommerce online store laptop business, dropshipping product packaging shipping, online sales entrepreneur"
    
    # Investimenti / Finanza / Trading
    if any(w in ctx for w in ["invest", "finanza", "trading", "azioni", "borsa", "portfolio"]):
        return "financial investment laptop charts graphs, stock trading analysis business, investment portfolio management"
    
    # Business / Imprenditoria / Startup
    if any(w in ctx for w in ["business", "impresa", "startup", "azienda", "imprenditor"]):
        return "entrepreneur business meeting laptop office, startup team working strategy, business growth success"
    
    # Money / Soldi / Guadagno
    if any(w in ctx for w in ["soldi", "money", "guadagno", "euro", "dollari", "ricchezza"]):
        return "money cash counting laptop calculator, financial success wealth business, income revenue profit"
    
    # Corsi / Formazione / Infoprodotti
    if any(w in ctx for w in ["corso", "formazione", "infoprodotto", "ebook", "membership"]):
        return "online course education laptop learning, digital product creation business, teaching online platform"
    
    # Lavoro da Casa / Remote / Online
    if any(w in ctx for w in ["casa", "remote", "online", "casa", "distanza"]):
        return "work from home laptop desk office, remote working home setup, online job computer workspace"
    
    # Se abbiamo keywords specifiche dallo Sheet
    if kw and kw != "none":
        return f"{kw}, passive income business, money entrepreneur laptop, side hustle freelance online work"
    
    # Fallback MONEY/BUSINESS generico
    return base

def is_rendite_video_metadata(video_data, source):
    """üîß Filtro BUSINESS/MONEY - NO banned (animals/nature) + keywords money/business"""
    business_keywords = ["money", "business", "laptop", "computer", "office", "work", "entrepreneur", 
                         "finance", "investment", "cash", "coins", "calculator", "desk", "typing", 
                         "keyboard", "screen", "meeting", "professional", "corporate", "workspace",
                         "freelance", "startup", "success", "growth", "strategy", "planning"]
    banned = ["dog", "cat", "animal", "pet", "cooking", "food", "gym", "fitness", "sports", 
              "nature", "forest", "mountain", "beach", "ocean", "wildlife", "wedding", "party"]
    
    if source == "pexels":
        text = (video_data.get("description", "") + " " + " ".join(video_data.get("tags", []))).lower()
    else:
        text = " ".join(video_data.get("tags", [])).lower()
    
    business_count = sum(1 for kw in business_keywords if kw in text)
    has_banned = any(kw in text for kw in banned)
    
    if has_banned:
        status = "‚ùå BANNED"
    elif business_count >= 1:
        status = f"‚úÖ BUSINESS({business_count})"
    else:
        status = f"‚ö†Ô∏è NEUTRAL(biz:{business_count})"
    
    print(f"üîç [{source}] '{text[:60]}...' ‚Üí {status}", flush=True)
    return not has_banned

def download_file(url: str) -> str:
    tmp_clip = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
    clip_resp = requests.get(url, stream=True, timeout=30)
    clip_resp.raise_for_status()
    for chunk in clip_resp.iter_content(chunk_size=1024 * 1024):
        if chunk:
            tmp_clip.write(chunk)
    tmp_clip.close()
    return tmp_clip.name

def fetch_clip_for_scene(scene_number: int, query: str, avg_scene_duration: float):
    """üéØ Canale RENDITE: B-roll business/money. Fallback Pixabay se Pexels 0."""
    target_duration = min(4.0, avg_scene_duration)
    
    def try_pexels():
        if not PEXELS_API_KEY:
            return None
        headers = {"Authorization": PEXELS_API_KEY}
        params = {
            "query": f"{query} money business entrepreneur laptop online work",
            "orientation": "landscape",
            "per_page": 25,
            "page": random.randint(1, 3),
        }
        resp = requests.get("https://api.pexels.com/videos/search", headers=headers, params=params, timeout=20)
        if resp.status_code != 200:
            return None
        videos = resp.json().get("videos", [])
        business_videos = [v for v in videos if is_rendite_video_metadata(v, "pexels")]
        print(f"üéØ Pexels: {len(videos)} totali ‚Üí {len(business_videos)} OK (no banned)", flush=True)
        if business_videos:
            video = random.choice(business_videos)
            for vf in video.get("video_files", []):
                if vf.get("width", 0) >= 1280:
                    return download_file(vf["link"])
        return None
    
    def try_pixabay():
        if not PIXABAY_API_KEY:
            return None
        params = {
            "key": PIXABAY_API_KEY,
            "q": f"{query} money business entrepreneur laptop online work",
            "per_page": 25,
            "safesearch": "true",
            "min_width": 1280,
        }
        resp = requests.get("https://pixabay.com/api/videos/", params=params, timeout=20)
        if resp.status_code != 200:
            return None
        hits = resp.json().get("hits", [])
        for hit in hits:
            if is_rendite_video_metadata(hit, "pixabay"):
                videos = hit.get("videos", {})
                for quality in ["large", "medium", "small"]:
                    if quality in videos and "url" in videos[quality]:
                        return download_file(videos[quality]["url"])
        return None
    
    for source_name, func in [("Pexels", try_pexels), ("Pixabay", try_pixabay)]:
        try:
            path = func()
            if path:
                print(f"üé• Scena {scene_number}: '{query[:40]}...' ‚Üí {source_name} ‚úì", flush=True)
                return path, target_duration
        except Exception as e:
            print(f"‚ö†Ô∏è {source_name}: {e}", flush=True)
    
    print(f"‚ö†Ô∏è NO CLIP per scena {scene_number}: '{query}'", flush=True)
    return None, None

@app.route("/health", methods=["GET"])
def health():
    return jsonify({"status": "healthy", "jobs": len(jobs)})

@app.route("/ffmpeg-test", methods=["GET"])
def ffmpeg_test():
    result = subprocess.run(["ffmpeg", "-version"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
    firstline = result.stdout.splitlines()[0] if result.stdout else "no output"
    return jsonify({"ffmpeg_output": firstline})

@app.route("/status/<job_id>", methods=["GET"])
def get_status(job_id):
    job = jobs.get(job_id)
    if not job:
        return jsonify({"error": "Job not found"}), 404
    
    response = {
        "job_id": job_id,
        "status": job["status"],
        "created_at": job.get("created_at")
    }
    if job['status'] == 'completed':
        response['video_url'] = job.get('video_url')
        response['duration'] = job.get('duration')
        response['clips_used'] = job.get('clips_used')
    elif job['status'] == 'failed':
        response['error'] = job.get('error')
    
    return jsonify(response)

def process_video_async(job_id, data):
    """Processa video in background thread"""
    job = jobs[job_id]
    job["status"] = "processing"
    job["job_id"] = job_id
    job["data"] = data
    
    audiopath = None
    audio_wav_path = None
    video_looped_path = None
    final_video_path = None
    scene_paths = []
    
    try:
        if not all([R2_ACCESS_KEY_ID, R2_SECRET_ACCESS_KEY, R2_BUCKET_NAME, R2_PUBLIC_BASE_URL]):
            raise RuntimeError("Config R2 mancante")
        
        audiobase64 = data.get("audio_base64") or data.get("audiobase64")
        raw_script = (data.get("script") or data.get("script_chunk") or data.get("script_audio") or data.get("script_completo") or "")
        script = (" ".join(str(p).strip() for p in raw_script) if isinstance(raw_script, list) else str(raw_script).strip())
        raw_keywords = data.get("keywords", "")
        sheet_keywords = (", ".join(str(k).strip() for k in raw_keywords) if isinstance(raw_keywords, list) else str(raw_keywords).strip())
        
        row_number_raw = data.get("row_number")
        if isinstance(row_number_raw, dict):
            row_number = int(row_number_raw.get('row', row_number_raw.get('row_number', 1)))
        elif isinstance(row_number_raw, str):
            row_number = int(row_number_raw) if row_number_raw.isdigit() else 1
        elif isinstance(row_number_raw, (int, float)):
            row_number = int(row_number_raw)
        else:
            row_number = 1

        print("=" * 80, flush=True)
        print(f"üí∞ START RENDITE E GUADAGNI EXTRA: {len(script)} char script, keywords: '{sheet_keywords}', row: {row_number}", flush=True)
        print(f"üîç DEBUG row_number RAW: '{row_number_raw}' ‚Üí PARSED: '{row_number}'", flush=True)
        print(f"üîç DEBUG GOOGLE_CREDENTIALS_JSON: {'PRESENTE ({len(GOOGLE_CREDENTIALS_JSON)} char)' if GOOGLE_CREDENTIALS_JSON else 'MANCANTE'}", flush=True)
        
        if not audiobase64:
            raise RuntimeError("audiobase64 mancante")
        
        # Audio processing
        audio_bytes = base64.b64decode(audiobase64)
        with tempfile.NamedTemporaryFile(delete=False, suffix=".bin") as f:
            f.write(audio_bytes)
        audiopath_tmp = f.name
        
        audio_wav_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        audio_wav_path = audio_wav_tmp.name
        audio_wav_tmp.close()
        
        subprocess.run([
            "ffmpeg", "-y", "-loglevel", "error", "-i", audiopath_tmp,
            "-acodec", "pcm_s16le", "-ar", "48000", audio_wav_path
        ], timeout=MAX_DURATION, check=True)
        os.unlink(audiopath_tmp)
        audiopath = audio_wav_path
        
        probe = subprocess.run([
            "ffprobe", "-v", "error", "-show_entries", "format=duration",
            "-of", "default=noprint_wrappers=1:nokey=1", audiopath
        ], stdout=subprocess.PIPE, text=True, timeout=10)
        real_duration = float(probe.stdout.strip() or 720.0)
        print(f"‚è±Ô∏è Durata audio: {real_duration/60:.1f}min ({real_duration:.0f}s)", flush=True)
        
        script_words = script.lower().split()
        words_per_second = (len(script_words) / real_duration if real_duration > 0 else 2.5)
        num_scenes = MAX_CLIPS
        avg_scene_duration = real_duration / num_scenes
        scene_assignments = []
        
        for i in range(num_scenes):
            if i % 10 == 0:
                print(f"üîß Clip {i}/{num_scenes}", flush=True)
            timestamp = i * avg_scene_duration
            word_index = int(timestamp * words_per_second)
            scene_context = " ".join(script_words[word_index: word_index + 7]) if word_index < len(script_words) else "money business passive income entrepreneur"
            scene_query = pick_visual_query(scene_context, sheet_keywords)
            scene_assignments.append({
                "scene": i + 1, "timestamp": round(timestamp, 1),
                "context": scene_context[:60], "query": scene_query[:80]
            })
        
        for assignment in scene_assignments:
            clip_path, clip_dur = fetch_clip_for_scene(
                assignment["scene"], assignment["query"], avg_scene_duration
            )
            if clip_path and clip_dur:
                scene_paths.append((clip_path, clip_dur))
        
        print(f"‚úÖ CLIPS SCARICATE: {len(scene_paths)}/{num_scenes}", flush=True)
        if len(scene_paths) < 5:
            raise RuntimeError(f"Troppe poche clip: {len(scene_paths)}/{num_scenes}")
        
        normalized_clips = []
        for i, (clip_path, _dur) in enumerate(scene_paths):
            try:
                normalized_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
                normalized_path = normalized_tmp.name
                normalized_tmp.close()
                subprocess.run([
                    "ffmpeg", "-y", "-loglevel", "error", "-i", clip_path,
                    "-vf", "scale=1920:1080:force_original_aspect_ratio=increase,crop=1920:1080,fps=30",
                    "-c:v", "libx264", "-preset", "fast", "-crf", "23", "-an", normalized_path
                ], timeout=MAX_DURATION, check=True)
                if os.path.exists(normalized_path) and os.path.getsize(normalized_path) > 1000:
                    normalized_clips.append(normalized_path)
            except Exception:
                pass
        
        if not normalized_clips:
            raise RuntimeError("Nessuna clip normalizzata")
        
        def get_duration(p):
            out = subprocess.run([
                "ffprobe", "-v", "error", "-show_entries", "format=duration",
                "-of", "default=noprint_wrappers=1:nokey=1", p
            ], stdout=subprocess.PIPE, text=True, timeout=10).stdout.strip()
            return float(out or 4.0)
        
        total_clips_duration = sum(get_duration(p) for p in normalized_clips)
        concat_list_tmp = tempfile.NamedTemporaryFile(mode="w", delete=False, suffix=".txt")
        entries_written = 0
        MAX_CONCAT_ENTRIES = 150
        
        if total_clips_duration < real_duration and len(normalized_clips) > 1:
            loops_needed = math.ceil(real_duration / total_clips_duration)
            for _ in range(loops_needed):
                for norm_path in normalized_clips:
                    if entries_written >= MAX_CONCAT_ENTRIES:
                        break
                    concat_list_tmp.write(f"file '{norm_path}'\n")
                    entries_written += 1
                if entries_written >= MAX_CONCAT_ENTRIES:
                    break
        else:
            for norm_path in normalized_clips:
                concat_list_tmp.write(f"file '{norm_path}'\n")
                entries_written += 1
        
        concat_list_tmp.close()
        
        video_looped_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
        video_looped_path = video_looped_tmp.name
        video_looped_tmp.close()
        
        subprocess.run([
            "ffmpeg", "-y", "-loglevel", "error",
            "-f", "concat", "-safe", "0", "-i", concat_list_tmp.name,
            "-vf", "fps=30,format=yuv420p", "-c:v", "libx264", "-preset", "fast", "-crf", "23",
            "-t", str(real_duration), video_looped_path
        ], timeout=MAX_DURATION, check=True)
        os.unlink(concat_list_tmp.name)
        
        final_video_tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp4")
        final_video_path = final_video_tmp.name
        final_video_tmp.close()
        
        subprocess.run([
            "ffmpeg", "-y", "-loglevel", "error",
            "-i", video_looped_path, "-i", audiopath,
            "-filter_complex", "[0:v]scale=1920:1080:force_original_aspect_ratio=increase,crop=1920:1080,format=yuv420p[v]",
            "-map", "[v]", "-map", "1:a", "-c:v", "libx264", "-preset", "fast", "-crf", "23",
            "-c:a", "aac", "-b:a", "192k", "-shortest", final_video_path
        ], timeout=MAX_DURATION, check=True)
        
        s3_client = get_s3_client()
        today = dt.datetime.utcnow().strftime("%Y-%m-%d")
        object_key = f"videos/{today}/{uuid.uuid4().hex}.mp4"
        s3_client.upload_file(
            Filename=final_video_path,
            Bucket=R2_BUCKET_NAME,
            Key=object_key,
            ExtraArgs={"ContentType": "video/mp4"}
        )
        public_url = f"{R2_PUBLIC_BASE_URL.rstrip('/')}/{object_key}"
        cleanup_old_videos(s3_client, object_key)
        
        gc = get_gspread_client()
        print(f"üîç DEBUG gspread client: {'OK' if gc else 'FAILED'}", flush=True)
        if gc and row_number > 0:
            try:
                sheet = gc.open_by_key(SPREADSHEET_ID).sheet1
                sheet.update_cell(row_number, 13, public_url)
                sheet.update_cell(row_number, 2, "PRODOTTO")
                print(f"üìä ‚úÖ Sheet row {row_number}: M={public_url[:60]} + B=PRODOTTO (anti-loop)", flush=True)
            except Exception as e:
                print(f"‚ùå Sheets fallito row {row_number}: {str(e)}", flush=True)
        
        paths_to_cleanup = [audiopath, video_looped_path, final_video_path] + normalized_clips + [p[0] for p in scene_paths]
        for path in paths_to_cleanup:
            try:
                os.unlink(path)
            except Exception:
                pass
        
        print(f"‚úÖ üí∞ VIDEO RENDITE E GUADAGNI EXTRA COMPLETO: {real_duration/60:.1f}min ‚Üí {public_url}", flush=True)
        
        job.update({
            "status": "completed",
            "video_url": public_url,
            "duration": real_duration,
            "clips_used": len(scene_paths),
            "row_number": row_number
        })

        # Notifica n8n flusso 2
        notify_n8n_flusso2(job)
        
    except Exception as e:
        print(f"‚ùå ERRORE PROCESSING: {e}", flush=True)
        job.update({"status": "failed", "error": str(e)})
    
    finally:
        Thread(target=lambda: cleanup_job_delayed(job_id), daemon=True).start()

def cleanup_job_delayed(job_id, delay=3600):
    import time
    time.sleep(delay)
    if job_id in jobs:
        del jobs[job_id]

@app.route("/generate", methods=["POST"])
def generate():
    try:
        data = request.get_json(force=True) or {}
        job_id = str(uuid.uuid4())
        
        jobs[job_id] = {
            "status": "queued",
            "created_at": dt.datetime.utcnow().isoformat(),
            "data": data
        }
        
        if len(jobs) > MAX_JOBS:
            old_jobs = sorted(jobs.keys(), key=lambda k: jobs[k]["created_at"])[:len(jobs)-MAX_JOBS]
            for oj in old_jobs:
                del jobs[oj]
        
        Thread(target=process_video_async, args=(job_id, data), daemon=True).start()
        
        print(f"üöÄ Job {job_id} QUEUED: raw_row={data.get('row_number')}", flush=True)
        return jsonify({
            "success": True,
            "job_id": job_id,
            "status": "queued",
            "message": "Video generation started (check /status/<job_id>)"
        })
    
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8080))
    app.run(host="0.0.0.0", port=port, threaded=True)
